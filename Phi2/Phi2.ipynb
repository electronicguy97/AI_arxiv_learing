{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmQjKjcH8dEV"
      },
      "source": [
        "# Phi-2\n",
        "- Microsoft 가 발표한 Tiny LLM(???)\n",
        "- 2.7B(27억) parameters\n",
        "- https://huggingface.co/microsoft/phi-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsYINNxR5W0E"
      },
      "outputs": [],
      "source": [
        "# ### install (upgrade) huggingface transformers library.\n",
        "# !pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P_5amjDhAG2y"
      },
      "outputs": [],
      "source": [
        "# 노트북에 보기 좋게 출력하기 위한 Utillity\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "import textwrap\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WEuZUOAf48A6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "\n",
        "torch.set_default_device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y82cnjX99IUA"
      },
      "source": [
        "- 몇몇 모델은 License 문제로 huggingface API key를 요구 할 수도 있으나 Phi-2 는 완전 공개 (MIT license) 모델이라 필요치 않음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GUpsQ-Sh5B02"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb05293fcfb24cc6b70dc674a339735c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/863 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\users\\zpdlc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zpdlc\\.cache\\huggingface\\hub\\models--microsoft--phi-2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e726ad5f4a9f418faac48067f7e2ffcf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "configuration_phi.py:   0%|          | 0.00/9.26k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
            "- configuration_phi.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57726bdc4afa436abf3c877048abe286",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modeling_phi.py:   0%|          | 0.00/62.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
            "- modeling_phi.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b562b3ea29e84a059e05c33aeb486cad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ade2f68e9064f8b83fd33e042d7bae2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2945978d3f5946a0898f924dea8221f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4793ae241ceb4548b7ceea8d1e21205c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d45d3767068d4294ab507cb8f1eb543c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fafd7049502482d9c310e9f53cec9fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f25c183e073c41ce9500eedbe3b13ee5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23c3e5ca8c0643aeb362461898daab77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed2425e8bb8c4e649486b8c56996de3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "015dc2a89c7a4527aeef951abe0e074d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3112c92fbb174a6c934c0131490ae814",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16218c8760ef4507ba6c3d1ce9943f5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SIdvQed9Wy1"
      },
      "source": [
        "- Model card 의 예제는 greedy generation\n",
        "- https://huggingface.co/docs/transformers/main/en/generation_strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TTHrwa3g5FGx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "> ```python\n",
              "> def print_prime(n):\n",
              ">    \"\"\"\n",
              ">    Print all primes between 1 and n\n",
              ">    \"\"\"\n",
              ">    for num in range(2, n+1):\n",
              ">        for i in range(2, num):\n",
              ">            if (num % i) == 0:\n",
              ">                break\n",
              ">        else:\n",
              ">            print(num)\n",
              "> \n",
              "> print_prime(20)\n",
              "> ```\n",
              "> \n",
              "> ## Exercises\n",
              "> \n",
              "> 1. Write a Python function that takes a list of numbers and returns the sum of all the even numbers in the list.\n",
              "> \n",
              "> ```python\n",
              "> def sum_even(numbers):\n",
              ">    \"\"\"\n",
              ">    Returns the sum of all even numbers in a list\n",
              ">    \"\"\"\n",
              ">    total = 0\n",
              ">    for num in numbers:\n",
              ">        if num % 2 == 0:\n",
              ">            total += num\n",
              ">    return total\n",
              "> \n",
              "> print(sum_even([1, 2, 3, 4, 5, 6]))\n",
              "> ```\n",
              "> \n",
              "> 2. Write a Python function that takes a list of strings and returns a new list containing only the"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cfg = GenerationConfig(\n",
        "    max_new_tokens=200,\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    top_k=40,\n",
        "    top_p=0.95,\n",
        "    eos_token_id=model.config.eos_token_id # stop_ token과 비슷한 기능\n",
        ")\n",
        "\n",
        "inputs = tokenizer('''```python\n",
        "def print_prime(n):\n",
        "   \"\"\"\n",
        "   Print all primes between 1 and n\n",
        "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "outputs = model.generate(**inputs, generation_config=cfg)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "to_markdown(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrCbIEvUD6j8"
      },
      "source": [
        "- Local 에서 LLM 을 구동시키고 API 를 제공하기위한 패키지들이 존재함\n",
        "  - https://github.com/abetlen/llama-cpp-python\n",
        "  - https://github.com/vllm-project/vllm\n",
        "\n",
        "- 모델을 더 작고 가볍게 만들어 낮은 성능의 PC에서도 실행 할 수 있게 하는 quantization 이라는 방법이 있음\n",
        "  - https://huggingface.co/blog/4bit-transformers-bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n"
          ]
        }
      ],
      "source": [
        "def sum_even(numbers):\n",
        "   \"\"\"\n",
        "   Returns the sum of all even numbers in a list\n",
        "   \"\"\"\n",
        "   total = 0\n",
        "   for num in numbers:\n",
        "       if num % 2 == 0:\n",
        "           total += num\n",
        "   return total\n",
        "\n",
        "print(sum_even([1, 2, 3, 4, 5, 6]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
